{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zR6aee__ckkD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras import datasets, layers, models,Model\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from multiprocessing import Pool\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iWfBK2cLj_Cv"
      },
      "outputs": [],
      "source": [
        "def load_samples():\n",
        "    data = pd.read_csv(\"csv\\sampled\\ct_train_final.csv\",index_col=0)  \n",
        "    c=0\n",
        "    for row in data.itertuples(index=False):\n",
        "      samples.append([x for x in row])\n",
        "      c+=1\n",
        "      # if(c==): # to break when required sample is already pulled\n",
        "      #   break\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mCr_d96o7IB",
        "outputId": "2e373e03-9004-40df-b950-9dfe6417a320"
      },
      "outputs": [],
      "source": [
        "samples=[]\n",
        "samples=load_samples()\n",
        "print (samples[0:5])\n",
        "print(len(samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z17y-1jYTPzO"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model=models.Sequential()\n",
        "  model.add(layers.Conv2D(16,kernel_size=(3, 3), activation='relu', input_shape=(275,275,1)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  # normliztion to use overfitting use drop out only in full connected network ie dense layer\n",
        "  \n",
        "  model.add(layers.BatchNormalization())\n",
        "\n",
        "  model.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "  model.add(layers.BatchNormalization())\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Dense(16, activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "  model.add(layers.Dense(5,activation='sigmoid'))\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "be7QDcKFdLw6"
      },
      "outputs": [],
      "source": [
        "bone_model=create_model()\n",
        "brain_model=create_model()\n",
        "blood_model=create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4CtyNIJZAk1D"
      },
      "outputs": [],
      "source": [
        "combined=layers.concatenate([bone_model.output,brain_model.output,blood_model.output])\n",
        "combined_model = layers.Dense(15, activation=\"relu\")(combined)\n",
        "combined_model = layers.Dense(5, activation=\"sigmoid\")(combined_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x_e1nUZQAAVm"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=[bone_model.input, brain_model.input,blood_model.input], outputs=combined_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2pd0U8_IoRn"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrrEG7l1H4rh",
        "outputId": "f4f58c0b-abcd-41e4-b4b9-e1cac69ae34c"
      },
      "outputs": [],
      "source": [
        "label.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C1Jeh7MNpf2O"
      },
      "outputs": [],
      "source": [
        "#Changing file directory and patience set to 5 & saving only best models\n",
        "filepath=\"models\\weights-improvement-{epoch:02d}-{accuracy:02f}.hdf5\"\n",
        "earlystop=EarlyStopping(monitor='accuracy', mode='max',  patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', save_best_only=True, mode='max')\n",
        "callbacks_list = [earlystop,checkpoint]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ldol3gqpCX1C"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Use this generator functions\n",
        "def generator(samples, batch_size=32):\n",
        "    \"\"\"\n",
        "    Yields the next training batch.\n",
        "    Suppose `samples` is an array [[image1_filename,label1], [image2_filename,label2],...].\n",
        "    \"\"\"\n",
        "    num_samples = len(samples)\n",
        "    while True: # Loop forever so the generator never terminates\n",
        "        shuffle(samples)\n",
        " \n",
        "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size &lt;= num_samples]\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            # Get the samples you'll use in this batch\n",
        "            batch_samples = samples[offset:offset+batch_size]\n",
        " \n",
        "            # Initialise X_train and y_train arrays for this batch\n",
        "            X_train = []\n",
        "            X_trains=[]\n",
        "            X_trainss=[]\n",
        "            y_train = []\n",
        "            \n",
        "            # For each example\n",
        "            for batch_sample in batch_samples:\n",
        "                # Load image (X)\n",
        "                filename = 'data\\\\processed\\\\bone_window\\\\'+batch_sample[0]+'.png'\n",
        "                image = cv2.imread(filename)\n",
        "                image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Read label (y)\n",
        "                y = [x for x in batch_sample[2:]]\n",
        "                X_train.append(image)\n",
        "                y_train.append(y)\n",
        "\n",
        "                filename = 'data\\\\processed\\\\brain_window\\\\'+batch_sample[0]+'.png'\n",
        "                image = cv2.imread(filename)\n",
        "                image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                X_trains.append(image)\n",
        "\n",
        "                filename = 'data\\\\processed\\\\blood_window\\\\'+batch_sample[0]+'.png'\n",
        "                image = cv2.imread(filename)\n",
        "                image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                X_trainss.append(image)\n",
        " \n",
        "            # Make sure they're numpy arrays (as opposed to lists)\n",
        "            X_train = np.array(X_train)/255.0\n",
        "            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "\n",
        "            X_trains = np.array(X_trains)/255.0\n",
        "            X_trains = X_trains.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "\n",
        "            X_trainss = np.array(X_trainss)/255.0\n",
        "            X_trainss = X_trainss.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "\n",
        "\n",
        "            # X_train=tf.convert/_to_tensor(X_train)\n",
        "            y_train = np.array(y_train)\n",
        "            # y_train=tf.convert_to_tensor(y_train)\n",
        "            # X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
        "            # The generator-y part: yield the next training batch            \n",
        "            yield [X_train,X_trains,X_trainss], y_train\n",
        "\n",
        "            \n",
        "train_datagen = generator(samples[0:int(len(samples)*0.8)],batch_size=32)\n",
        "validation_generator = generator(samples[int(len(samples)*0.8):], batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHX74k98CZ1s",
        "outputId": "0d89e533-7ffa-4e9a-ccaa-1b1dba5a82f8"
      },
      "outputs": [],
      "source": [
        "# Model Training Cell\n",
        "# batch=32\n",
        "# steps_per_epoch = TotalTrainingSamples / TrainingBatchSize\n",
        "# validation_steps = TotalvalidationSamples / ValidationBatchSize\n",
        "history=model.fit(\n",
        "        train_datagen,\n",
        "        epochs=11, steps_per_epoch=int(len(samples)*0.8/32)\n",
        "        ,validation_steps=int(0.2 * len(samples)/32)\n",
        "        ,callbacks=callbacks_list\n",
        "        ,validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/temp/weights-improvement-06-0.633729.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuksvSACtcEP",
        "outputId": "72655d5e-2a06-42ea-de25-1af9231571b9"
      },
      "outputs": [],
      "source": [
        "filename = '/content/drive/MyDrive/processed/bone_window/'+'ID_000a050f3'+'.png'              \n",
        "X_train = []\n",
        "image = cv2.imread(filename)\n",
        "image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "X_train.append(image)\n",
        "X_train = np.array(X_train)/255.0\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "\n",
        "filenames = '/content/drive/MyDrive/processed/brain_window/'+'ID_000a050f3'+'.png'             \n",
        "X_trains = []\n",
        "image = cv2.imread(filename)\n",
        "image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "X_trains.append(image)\n",
        "X_trains = np.array(X_train)/255.0\n",
        "X_trains = X_trains.reshape(X_trains.shape[0], X_trains.shape[1], X_trains.shape[2], 1)\n",
        "\n",
        "filenames = '/content/drive/MyDrive/processed/blood_window/'+'ID_000a050f3'+'.png'\n",
        "X_trainss = []\n",
        "image = cv2.imread(filenames)\n",
        "image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "X_trainss.append(image)\n",
        "X_trainss = np.array(X_trainss)/255.0\n",
        "X_trainss = X_trainss.reshape(X_trains.shape[0], X_trains.shape[1], X_trains.shape[2], 1)\n",
        "\n",
        "model.predict([X_train,X_trains,X_trainss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "3oGaI8sRuAfM",
        "outputId": "697258d7-dbee-44aa-8e8b-e9bc6f11b669"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7vv_0sWquYnZ",
        "outputId": "31dc007e-8cf8-4ff2-f244-dad614b52ba5"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "UNUSED CELLS BELOW>>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jjuG4dnpv6Sq"
      },
      "outputs": [],
      "source": [
        "#Dont Use This Cell \n",
        "#Without using Pool and multiprocessing. Doesnt cause Memory overload\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Resources/ct_train_final.csv\",index_col=0)\n",
        "samples=[]  \n",
        "c=0\n",
        "bone_input=[]\n",
        "brain_input=[]\n",
        "blood_input=[]\n",
        "label=[]\n",
        "for row in data.itertuples(index=False):\n",
        "    samples.append([x for x in row])\n",
        "    c+=1\n",
        "    if(c==6000):\n",
        "      break\n",
        "for batch_sample in samples:\n",
        "                # Load image (X)\n",
        "  filename_bone = '/content/processed/bone_window/'+batch_sample[0]+'.png'\n",
        "  filename_brain = '/content/processed/brain_window/'+batch_sample[0]+'.png'\n",
        "  filename_blood = '/content/processed/blood_window/'+batch_sample[0]+'.png'\n",
        "  image = cv2.imread(filename_bone)\n",
        "  image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  bone_input.append(image)\n",
        "\n",
        "  # y = [x for x in batch_sample[2:]]\n",
        "  image = cv2.imread(filename_brain)\n",
        "  image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  brain_input.append(image)\n",
        "\n",
        "  image = cv2.imread(filename_blood)\n",
        "  image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  blood_input.append(image)\n",
        "\n",
        "  label.append([x for x in batch_sample[2:]])\n",
        "bone_input = np.array(bone_input)/255.0\n",
        "bone_input = bone_input.reshape(bone_input.shape[0], bone_input.shape[1], bone_input.shape[2], 1)\n",
        "\n",
        "brain_input = np.array(brain_input)/255.0\n",
        "brain_input = brain_input.reshape(brain_input.shape[0], brain_input.shape[1], brain_input.shape[2], 1)\n",
        "\n",
        "blood_input = np.array(blood_input)/255.0\n",
        "blood_input = blood_input.reshape(brain_input.shape[0], brain_input.shape[1], brain_input.shape[2], 1)\n",
        "\n",
        "# m=tf.convert_to_tensor(m)\n",
        "label= np.array(label)\n",
        "# n=tf.convert_to_tensor(n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#It is one of two cell to load images faster using multiprocessing if upper cell is executed no need to run\n",
        "#Part of Pool Cell\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Resources/ct_train_final.csv\",index_col=0)\n",
        "samples=[]  \n",
        "c=0\n",
        "bone_input=[]\n",
        "brain_input=[]\n",
        "blood_input=[]\n",
        "label=[]\n",
        "for row in data.itertuples(index=False):\n",
        "    samples.append([x for x in row])\n",
        "    c+=1\n",
        "    # if(c==5):\n",
        "    #   break\n",
        "\n",
        "def load_bone_images(samples):\n",
        "  filename_bone = '/content/processed/bone_window/'+samples[0]+'.png'\n",
        "  image = cv2.imread(filename_bone)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  return image\n",
        "\n",
        "def load_brain_images(samples):\n",
        "  filename_bone = '/content/processed/brain_window/'+samples[0]+'.png'\n",
        "  image = cv2.imread(filename_bone)\n",
        "  image=  cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  return image\n",
        "\n",
        "def load_blood_images(samples):\n",
        "  filename_bone = '/content/processed/blood_window/'+samples[0]+'.png'\n",
        "  image = cv2.imread(filename_bone)\n",
        "  image=  cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  return image\n",
        "\n",
        "def load_label(samples):\n",
        "  # label.append([x for x in batch_sample[2:]])\n",
        "  return [x for x in samples[2:]];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#It is the second portion to load images using multiprocessing\n",
        "#Using Pool causes Memory Overload\n",
        "\n",
        "\n",
        "bone_input=[]\n",
        "brain_input=[]\n",
        "blood_input=[]\n",
        "label=[]\n",
        "with Pool(2) as p:\n",
        "  bone_input.append(p.map(load_bone_images, samples))\n",
        "  bone_input=bone_input[0]\n",
        "  bone_input = np.array(bone_input)/255.0\n",
        "  bone_input = bone_input.reshape(bone_input.shape[0], bone_input.shape[1], bone_input.shape[2], 1)\n",
        "  p.close()\n",
        "\n",
        "with Pool(2) as p:\n",
        "  brain_input.append(p.map(load_brain_images, samples))\n",
        "  brain_input=brain_input[0]\n",
        "  brain_input = np.array(brain_input)/255.0\n",
        "  brain_input = brain_input.reshape(bone_input.shape[0], bone_input.shape[1], bone_input.shape[2], 1)\n",
        "  p.close()\n",
        "\n",
        "with Pool(2) as p:\n",
        "  blood_input.append(p.map(load_brain_images, samples))\n",
        "  blood_input=blood_input[0]\n",
        "  blood_input = np.array(blood_input)/255.0\n",
        "  blood_input = blood_input.reshape(brain_input.shape[0], brain_input.shape[1], brain_input.shape[2], 1)\n",
        "  p.close()\n",
        "\n",
        "with Pool(2) as p:\n",
        "  label.append(p.map(load_label,samples))\n",
        "  label=label[0]\n",
        "  label= np.array(label)\n",
        "  p.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Brain Haemorrhage CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
